{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開発環境・主要なライブラリ\n",
    "\n",
    "> Python3.7.8\n",
    "> \n",
    "> OpenCV (画像処理)\n",
    "> \n",
    "> Scikit-learn (機械学習)\n",
    "> \n",
    "> pickle (機械学習のモデルを保存読込)\n",
    "> \n",
    "> pyautogui (Unity環境のロボットを操作)\n",
    "> \n",
    "> matplotlib (画像認識の結果をグラフ表示)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import cv2\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pyautogui as pg\n",
    "\n",
    "from time import time, sleep\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange\n",
    "from numba import jit\n",
    "from pprint import pprint\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import svm, neural_network\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from threading import Thread\n",
    "from matplotlib import pyplot as plt\n",
    "from line_profiler import LineProfiler\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "CAMERA_SIZE = (50, 85)\n",
    "\n",
    "LEFT_ACTCODE = 0\n",
    "RIGHT_ACTCODE = 1\n",
    "LEFT90_ACTCODE = 2\n",
    "RIGHT90_ACTCODE = 3\n",
    "STRAIGHT_ACTCODE = 4\n",
    "\n",
    "\n",
    "def SaveInstances(instance, path):\n",
    "    '''\n",
    "    PickleモジュールのWrapper\n",
    "    instanceのオブジェクトをpathで指定したファイルパスに保存する。\n",
    "    '''\n",
    "    with open(path, mode='wb') as file:\n",
    "        pickle.dump(instance, file, protocol=2)\n",
    "\n",
    "\n",
    "def LoadInstances(path):\n",
    "    '''\n",
    "    PickleモジュールのWrapper\n",
    "    pathに保存されているpickle形式のファイルをオブジェクトとして読み込む。\n",
    "    '''\n",
    "    with open(path, 'rb') as ins:\n",
    "        return pickle.load(ins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 色識別の機械学習モデルをつくる\n",
    "\n",
    "### 機械学習モデル\n",
    "\n",
    "1. 非線形サポートベクターマシン(sklearn.svm.svc)\n",
    "2. パラメータの調整と最適化 = なし\n",
    "3. 説明変数 = 特定の色から取得したRGB値\n",
    "4. 目的変数 = 色\n",
    "\n",
    "### 学習工程\n",
    "\n",
    "1. 白黒緑のそれぞれの色を示す写真をカメラから取得する。(画面全体にその色が表示されていることが必須)\n",
    "2. 取得した画像に含まれるピクセルすべてをサンプルとして加工する。(RGB値を格納した1次元配列に変換する)\n",
    "3. ピクセルのRGB値を学習させる。\n",
    "\n",
    "### サポートベクターマシンとは\n",
    "\n",
    "教師あり学習の機械学習モデルの中でも分類に特化したモデル。学習時にサンプル分類の基準となる線(サポートベクターと呼ぶ)を生成し、未知のサンプルを分類するときにはその線との位置関係によって分類を行う。\n",
    "\n",
    "### 非線形サポートベクターマシン(SVC)を利用した理由\n",
    "\n",
    "SVMの利点はパラメータの調整と最適化が不要なことである。そして有名な機械学習モデルであるためドキュメントや解説記事が豊富に見つけられる。そのためSVMは便利なモデルであると言える。\n",
    "\n",
    "また、SVMは分類するクラス(目的変数)が多すぎると分析精度が下がり、スケーリング(数値の規模を統一すること)が必要という難点がある。しかしながら今回のような分類問題は、分類するクラスは白･黒･緑の3つであり、RGB値は0~255の範囲で表されるため、問題とはならないと考えた。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 白色のRGB値サンプルを取る\n",
    "\n",
    "ret, raw_frame = camera.read()\n",
    "frame = cv2.resize(src=raw_frame, dsize=CAMERA_SIZE)\n",
    "height, width = frame.shape[:2]\n",
    "\n",
    "WhiteRGB_sample = [frame[i, j] for i in range(height) for j in range(width)]\n",
    "\n",
    "img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "fig = plt.figure(1, (5., 5.))\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 黒色のRGB値サンプルを取る\n",
    "\n",
    "ret, raw_frame = camera.read()\n",
    "frame = cv2.resize(src=raw_frame, dsize=CAMERA_SIZE)\n",
    "height, width = frame.shape[:2]\n",
    "\n",
    "BlackRGB_sample = [frame[i, j] for i in range(height) for j in range(width)]\n",
    "\n",
    "img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "fig = plt.figure(1, (5., 5.))\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 緑色のRGB値サンプルを取る\n",
    "\n",
    "ret, raw_frame = camera.read()\n",
    "frame = cv2.resize(src=raw_frame, dsize=CAMERA_SIZE)\n",
    "height, width = frame.shape[:2]\n",
    "\n",
    "GreenRGB_sample = [frame[i, j] for i in range(height) for j in range(width)]\n",
    "\n",
    "img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "fig = plt.figure(1, (5., 5.))\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 色識別の分類モデルを作る(SVM)\n",
    "\n",
    "# それぞれの色に0,1,2の分類をつける\n",
    "WhiteRGB_sample = [[point[0], point[1], point[2], 0]\n",
    "                   for point in WhiteRGB_sample]\n",
    "BlackRGB_sample = [[point[0], point[1], point[2], 1]\n",
    "                   for point in BlackRGB_sample]\n",
    "GreenRGB_sample = [[point[0], point[1], point[2], 2]\n",
    "                   for point in GreenRGB_sample]\n",
    "\n",
    "allOf_samples = np.concatenate(\n",
    "    [WhiteRGB_sample, BlackRGB_sample, GreenRGB_sample])\n",
    "\n",
    "allOf_samples = LoadInstances(path=\"allOf_samples.pickles\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    allOf_samples[:, :-1],\n",
    "    allOf_samples[:, -1:],\n",
    "    stratify=allOf_samples[:, -1:],  # テストに使うデータを階層化する\n",
    "    test_size=0.3,  # テストに使う割合\n",
    ")\n",
    "\n",
    "model = svm.SVC().fit(X_train, y_train)\n",
    "\n",
    "SaveInstances(instance=model, path=\"ColorModel.pickles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 色識別の分類モデルをテストする\n",
    "\n",
    "model = LoadInstances(\"ColorModel.pickle\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, cmap='Blues', linewidth=0.3)\n",
    "ax.set_xlabel(\"Predicted\", fontsize=20)\n",
    "ax.set_ylabel(\"Accrual\", fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 色識別の分類モデルを使って、タイルを色分類してみる\n",
    "def func_Classification():\n",
    "    ret, raw_frame = camera.read()\n",
    "    frame = cv2.resize(src=raw_frame, dsize=CAMERA_SIZE)\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    result = [model.predict(row) for row in reversed(frame)]\n",
    "\n",
    "    plt.clf()\n",
    "    plt.pcolormesh(result)\n",
    "    plt.pause(0.01)\n",
    "\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    fig = plt.figure(1, (5., 5.))\n",
    "    plt.imshow(img)\n",
    "    plt.pause(0.01)\n",
    "\n",
    "\n",
    "lp = LineProfiler()\n",
    "lp.add_function(func_Classification)\n",
    "lp.runcall(func_Classification)\n",
    "lp.print_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## タイル分類の機械学習モデルをつくる\n",
    "\n",
    "### 機械学習モデル\n",
    "\n",
    "1. ニューラルネットワーク(多層パーセプトロン分類)\n",
    "2. パラメータの調整と最適化 = なし\n",
    "3. 説明変数 = カメラから取得したタイルの写真を色識別モデルで分類したもの\n",
    "4. 目的変数 = ロボットが進行すべき方向(左～前～右)\n",
    "\n",
    "### 学習工程\n",
    "\n",
    "1. 動画編集ソフトでタイルを模した動画を作成する(タイルの画像を乱数で移動回転させることであらゆる外乱を再現できる)\n",
    "2. 作成した動画からフレームごとに写真を抜き出し、それを色識別モデルで分類させる\n",
    "3. 色識別済みのタイル写真を学習させる。\n",
    "\n",
    "### ニューラルネットワークとは\n",
    "\n",
    "人間の脳神経細胞(ニューロン)を模した機械学習モデル。ニューロンは与えられた情報をもとに、他のニューロンに情報を出力する。ニューラルネットワーク全体を見ると入力層･処理層･出力層に分類することができ、入力層に説明変数を与え、処理層でニューロンが判断を行い、出力層から目的変数が返される。\n",
    "\n",
    "### ニューラルネットワークを利用した理由\n",
    "\n",
    "このモデルはパターン認識に特に適しており、画像認識や音声認識、時系列予測などにも利用されている。そのような認識問題はライントレースのタイル識別によく似ている問題である。\n",
    "\n",
    "また、多層パーセプトロンでは大量のニューロンが認識処理に関わるため、より複雑な処理が行なえる。したがってライントレースのタイル認識にも十分な性能を発揮できると考える。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# タイルのサンプルを集める\n",
    "# タイルのサンプルを集める (左に曲がる)\n",
    "Left_videoPath = \"C:/Users/Haya/OneDrive/デスクトップ/部活関係/ロボカップ2022/left_vid.mp4\"\n",
    "\n",
    "vid = cv2.VideoCapture(Left_videoPath)\n",
    "\n",
    "ret, frame = vid.read()\n",
    "if ret:\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    fig = plt.figure(1, (5., 5.))\n",
    "    plt.imshow(img)\n",
    "    plt.pause(0.1)\n",
    "\n",
    "# タイルのサンプルを集める (右に曲がる)\n",
    "Right_videoPath = \"C:/Users/Haya/OneDrive/デスクトップ/部活関係/ロボカップ2022/right_vid.mp4\"\n",
    "\n",
    "vid = cv2.VideoCapture(Right_videoPath)\n",
    "\n",
    "ret, frame = vid.read()\n",
    "if ret:\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    fig = plt.figure(1, (5., 5.))\n",
    "    plt.imshow(img)\n",
    "    plt.pause(0.1)\n",
    "\n",
    "# タイルのサンプルを集める (90度に左に曲がる)\n",
    "Left90_videoPath = \"C:/Users/Haya/OneDrive/デスクトップ/部活関係/ロボカップ2022/left90_vid.mp4\"\n",
    "\n",
    "vid = cv2.VideoCapture(Left90_videoPath)\n",
    "\n",
    "ret, frame = vid.read()\n",
    "if ret:\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    fig = plt.figure(1, (5., 5.))\n",
    "    plt.imshow(img)\n",
    "    plt.pause(0.1)\n",
    "\n",
    "# タイルのサンプルを集める (90度に右に曲がる)\n",
    "Right90_videoPath = \"C:/Users/Haya/OneDrive/デスクトップ/部活関係/ロボカップ2022/right90_vid.mp4\"\n",
    "\n",
    "vid = cv2.VideoCapture(Right90_videoPath)\n",
    "\n",
    "ret, frame = vid.read()\n",
    "if ret:\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    fig = plt.figure(1, (5., 5.))\n",
    "    plt.imshow(img)\n",
    "    plt.pause(0.1)\n",
    "\n",
    "# タイルのサンプルを集める (真っ直ぐ進む)\n",
    "Straight_videoPath = \"C:/Users/Haya/OneDrive/デスクトップ/部活関係/ロボカップ2022/stright_vid.mp4\"\n",
    "\n",
    "vid = cv2.VideoCapture(Straight_videoPath)\n",
    "\n",
    "ret, frame = vid.read()\n",
    "if ret:\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    fig = plt.figure(1, (5., 5.))\n",
    "    plt.imshow(img)\n",
    "    plt.pause(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライントレースの判断モデルを作る(NeuralNetwork)\n",
    "pathList = [\n",
    "    # それぞれの行動パターンに番号を振り与える\n",
    "    [Left_videoPath, LEFT_ACTCODE],\n",
    "    [Right_videoPath, RIGHT_ACTCODE],\n",
    "    [Left90_videoPath, LEFT90_ACTCODE],\n",
    "    [Right90_videoPath, RIGHT90_ACTCODE],\n",
    "    [Straight_videoPath, STRAIGHT_ACTCODE],\n",
    "]\n",
    "\n",
    "model = LoadInstances(\"ColorModel.pickle\")\n",
    "allOf_samples = np.empty((1, CAMERA_SIZE[0] * CAMERA_SIZE[1] + 1))\n",
    "\n",
    "for pl in pathList:\n",
    "    path = pl[0]\n",
    "    act = pl[1]\n",
    "    vid = cv2.VideoCapture(path)\n",
    "    frame_count = math.floor(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    count = 0\n",
    "    for f in tqdm(range(frame_count)):\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            ret, raw_frame = vid.read()\n",
    "            frame = cv2.resize(src=raw_frame, dsize=CAMERA_SIZE)\n",
    "            res = np.array([model.predict(row) for row in reversed(frame)])\n",
    "            res = np.append(res, act)\n",
    "            allOf_samples = np.append(allOf_samples, [res], axis=0)\n",
    "\n",
    "allOf_samples = allOf_samples[1:]\n",
    "pprint(allOf_samples)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    allOf_samples[:, :-1],\n",
    "    allOf_samples[:, -1:],\n",
    "    stratify=allOf_samples[:, -1:],  # テストに使うデータを階層化する\n",
    "    test_size=0.3,  # テストに使う割合\n",
    ")\n",
    "\n",
    "model = neural_network.MLPClassifier().fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライントレースの判断モデルを作る(NeuralNetwork without SVM-ColorClassify)\n",
    "pathList = [\n",
    "    # それぞれの行動パターンに番号を振り与える\n",
    "    [Left_videoPath, LEFT_ACTCODE],\n",
    "    [Right_videoPath, RIGHT_ACTCODE],\n",
    "    [Left90_videoPath, LEFT90_ACTCODE],\n",
    "    [Right90_videoPath, RIGHT90_ACTCODE],\n",
    "    [Straight_videoPath, STRAIGHT_ACTCODE],\n",
    "]\n",
    "\n",
    "allOf_samples = np.empty((1, (CAMERA_SIZE[0] * CAMERA_SIZE[1]) * 3 + 1))\n",
    "\n",
    "for pl in pathList:\n",
    "    path = pl[0]\n",
    "    act = pl[1]\n",
    "    vid = cv2.VideoCapture(path)\n",
    "    frame_count = math.floor(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    count = 0\n",
    "    for f in tqdm(range(frame_count)):\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            ret, raw_frame = vid.read()\n",
    "            frame = cv2.resize(src=raw_frame, dsize=CAMERA_SIZE)\n",
    "            res = np.ravel(frame)\n",
    "            res = np.append([res], act)\n",
    "            allOf_samples = np.append(allOf_samples, [res], axis=0)\n",
    "\n",
    "allOf_samples = allOf_samples[1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    allOf_samples[:, :-1],\n",
    "    allOf_samples[:, -1:],\n",
    "    stratify=allOf_samples[:, -1:],  # テストに使うデータを階層化する\n",
    "    test_size=0.3,  # テストに使う割合\n",
    ")\n",
    "\n",
    "model = neural_network.MLPClassifier().fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライントレースの判断モデルをテストする\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, cmap='Blues', linewidth=0.3)\n",
    "ax.set_xlabel(\"Predicted\", fontsize=20)\n",
    "ax.set_ylabel(\"Accrual\", fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライントレースの判断モデルを使ってタイルを見させてみる\n",
    "\n",
    "model_color = LoadInstances(\"ColorModel.pickle\")\n",
    "model_decision = LoadInstances(\"NeuralDecision.pickle\")\n",
    "\n",
    "# カメラで撮影する\n",
    "camera = cv2.VideoCapture(0)\n",
    "ret, raw_frame = camera.read()\n",
    "frame = cv2.resize(src=raw_frame, dsize=CAMERA_SIZE)\n",
    "height, width = frame.shape[:2]\n",
    "\n",
    "# カメラからの写真を表示する\n",
    "img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "fig = plt.figure(1, (5., 5.))\n",
    "plt.imshow(img)\n",
    "plt.pause(0.1)\n",
    "\n",
    "# 写真を色識別する\n",
    "result = [model_color.predict(row) for row in reversed(frame)]\n",
    "plt.clf()\n",
    "plt.pcolormesh(result)\n",
    "plt.pause(0.01)\n",
    "\n",
    "res = model_decision.predict([np.ravel(result)])[0]\n",
    "\n",
    "acttype = [\n",
    "    [LEFT_ACTCODE, \"LEFT\"],\n",
    "    [RIGHT_ACTCODE, \"RIGHT\"],\n",
    "    [LEFT90_ACTCODE, \"LEFT90\"],\n",
    "    [RIGHT90_ACTCODE, \"RIGHT90\"],\n",
    "    [STRAIGHT_ACTCODE, \"STRAIGHT\"],\n",
    "]\n",
    "\n",
    "for at in acttype:\n",
    "    if res == at[0]:\n",
    "        print(at[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep, time\n",
    "\n",
    "while True:\n",
    "    sleep(1 / 8)\n",
    "    print(time())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e8337386a09f3e9da107d82f3b7d4d547cb39f0fa1e32022713913d2edd8113"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
